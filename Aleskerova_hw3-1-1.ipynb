{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you need to solve four tasks. The assignment should be uploaded in Jupyter Notebook format(`.ipynb`). Overall, there are 23 points, however, you can get a maximum of 20 points for this assignment( `your_points = max(your_points, 20)`). No bonus points will be transferred to the next assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Non-parametric Density Estimation (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nla\n",
    "import scipy\n",
    "import scipy.stats as sps\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, LeaveOneOut\n",
    "from sklearn.neighbors import KernelDensity, NearestNeighbors\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Matplotlib settings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# non-interactive\n",
    "%matplotlib inline\n",
    "\n",
    "#jupyterlab\n",
    "# %matplotlib widget \n",
    "\n",
    "#jupyter-notebook\n",
    "#%matplotlib notebook \n",
    "\n",
    "titlesize = 20\n",
    "labelsize = 16\n",
    "legendsize = labelsize\n",
    "xticksize = 14\n",
    "yticksize = xticksize\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.5\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "matplotlib.rcParams['font.serif'] = 'Times New Roman'\n",
    "matplotlib.rcParams['axes.labelsize'] = labelsize\n",
    "matplotlib.rcParams['axes.titlesize'] = titlesize\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=xticksize)\n",
    "matplotlib.rc('ytick', labelsize=yticksize)\n",
    "matplotlib.rc('legend', fontsize=legendsize)\n",
    "\n",
    "matplotlib.rc('font', **{'family':'serif'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with data sampled from the mixture of Normal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = 31337\n",
    "seed2 = 42\n",
    "n_samples = 100\n",
    "\n",
    "f1 = scipy.stats.norm(loc=0, scale=1)\n",
    "f2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "\n",
    "Xa = f1.rvs(size=int(p1 * n_samples), random_state=seed1)\n",
    "Xb = f2.rvs(size=int(p2 * n_samples), random_state=seed2)\n",
    "samples = np.concatenate([Xa, Xb])\n",
    "\n",
    "a = -5\n",
    "b = 10\n",
    "\n",
    "x_values = np.linspace(a, b, 1000)\n",
    "binedges = np.linspace(a, b, 10)\n",
    "true_pdf = p1 * f1.pdf(x_values) + p2 * f2.pdf(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03689581576379748\n",
      "4.865287844298487\n",
      "0.9935411999787792\n",
      "0.7977395621619012\n",
      "-2.4302277402345944\n",
      "6.852278184508938\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(Xa))\n",
    "print(np.mean(Xb))\n",
    "print(np.var(Xa))\n",
    "print(np.var(Xb))\n",
    "print(np.min(samples))\n",
    "print(np.max(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\boldX}{\\boldsymbol{X}}$\n",
    "$\\newcommand{\\xs}[1]{\\boldX^{(#1)}}$\n",
    "$\\newcommand{\\Sum}{\\sum\\limits}$\n",
    "$\\newcommand{\\Int}{\\int\\limits}$\n",
    "$\\newcommand{\\hatp}{\\widehat{p}}$\n",
    "$\\newcommand{\\hatJ}{\\widehat{J}}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "\n",
    "Given a sample $\\boldsymbol{X}^{(n)} = \\boldsymbol{X}_1, \\boldsymbol{X}_2, ..., \\boldsymbol{X}_n \\sim \\; iid \\; p(x)$ we would like to build a hitogram estimate of the density. If we have $m$ bins of size $h$, we can have the following estimates of risk using leave-one-out cross validation:\n",
    "\n",
    "\\begin{gather*}\n",
    "J(h) = \\int \\hatp^2\\lp x;\\xs{n} \\rp dx - 2 \\int \\hatp \\lp x;\\xs{n} \\rp p(x) dx, \n",
    "\\end{gather*}\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\int \\hatp^2 \\lp x;\\xs{n} \\rp dx - \\frac{2}{n}\\Sum_{i = 1}^n \\hatp \\lp \\boldsymbol{X}_i ;\\xs{n\\backslash i} \\rp, \\label{eq1}\\tag{1}\n",
    "\\end{gather*}\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{2}{(n - 1)h} - \\frac{n+1}{(n-1)h}\\Sum_{i = 1}^m \\hatp_j^2,\\quad \\hatp_j = \\frac{n_j}{n}. \\label{eq2}\\tag{2}\n",
    "\\end{gather*}\n",
    "\n",
    "Your task is:\n",
    "1. Build a histogram estimate of the pdf given the sample above, tune bandwidth using leave-one-out CV. Use formula (\\ref{eq2}). Also check out `np.histogram` (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_count(X, h):\n",
    "    return np.int(np.ceil((np.max(X) - np.min(X)) / h))\n",
    "\n",
    "def get_bandwidth(X, bins_num):\n",
    "    return (np.max(X) - np.min(X)) / bins_num\n",
    "    \n",
    "def J_estimate(h, X):\n",
    "    n = len(X)\n",
    "    X_min, X_max = np.min(X), np.max(X)\n",
    "    bins_num = bins_count(X, h)\n",
    "    p =  np.histogram(X, bins = bins_num, range = (np.min(X), np.max(X)), density = False)[0]\n",
    "    summ = np.sum([(n_i / n)**2 for n_i in p])\n",
    "    return 2 / ((n-1) * h) - (n + 1) / ((n-1) * h) * summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8458334509090585 -0.16162171212089912 11\n"
     ]
    }
   ],
   "source": [
    "h_list = np.logspace(-2, 1, 1500)\n",
    "cv_list = [J_estimate(h_i, samples) for h_i in h_list]\n",
    "j_min = np.argmin(cv_list)\n",
    "h_opt = h_list[j_min]\n",
    "J_est_min = cv_list[j_min]\n",
    "\n",
    "bins_opt = bins_count(samples, h_opt)\n",
    "bw_opt = get_bandwidth(samples, bins_opt)\n",
    "    \n",
    "print(h_opt, J_est_min, bins_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\\* Prove that for histograms (2) follows from (1) (1 bonus point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have to prove that \n",
    "$$\n",
    "\\hatJ(h) = \\frac{2}{(n - 1)h} - \\frac{n+1}{(n-1)h}\\Sum_{i = 1}^m \\hatp_j^2,\\quad \\hatp_j = \\frac{n_j}{n}\n",
    "$$\n",
    "##### follows from\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\int \\hatp^2 \\lp x;\\xs{n} \\rp dx - \\frac{2}{n}\\Sum_{i = 1}^n \\hatp \\lp \\boldsymbol{X}_i ;\\xs{n\\backslash i} \\rp\n",
    "\\end{gather*}\n",
    "$$\n",
    "##### Consider first part of this expression:\n",
    "$$\n",
    "I_1 = \\int \\hat p^2(x; X^{n}) dx = \\int \\dfrac{1}{n^2 h^2} (\\sum \\limits_{i = 1}^{n} \\mu_i \\mathbb{1}(x \\in \\Delta_i))^2 = \\int \\dfrac{1}{n^2 h^2} \\sum \\limits_{i = 1}^{n} \\mu_i^2 \\mathbb{1}(x \\in \\Delta_i) = \\dfrac{1}{n^2 h^2} \\sum \\limits_{i = 0}^{n} \\mu_i^2 h,\n",
    "$$\n",
    "##### where $h$ is the size of intervals $\\Delta_i$ and $\\mu_i$ is the number of samples falling into  $\\Delta_i$. If $\\hat p_i = \\dfrac{\\mu_i}{n}$ - fraction of samples falling into  $\\Delta_i$, then\n",
    "$$\n",
    "I_1 = \\dfrac{1}{h} \\sum \\limits_{i = 1}^{n} \\hat p_i^2,\n",
    "$$\n",
    "##### Consider second part of this expression:\n",
    "$$\n",
    "I_2 = \\frac{1}{n}\\Sum_{i = 1}^n \\hatp (X_i; X^{n\\backslash i}) = \\frac{1}{n} \\dfrac{\\sum\\limits_{i=1}^{n} (\\mu_i^2 - 1)}{(n-1)h} = \\frac{1}{n} \\dfrac{\\sum\\limits_{i=1}^{n}\\mu_i^2 - n}{(n-1)h} = \\dfrac{n \\sum\\limits_{i=1}^{n} \\hat p_i^2 - 1}{(n-1)h}\n",
    "$$\n",
    "##### Here we have square of $\\mu_i$ because in this expression we sum over all samples (not bins), so each $\\mu_i$ appears  $\\mu_i$ times.\n",
    "##### Therefore,\n",
    "$$\n",
    "\\hat J(h) = I_1 - 2 I_2 = \\dfrac{1}{h} \\sum \\limits_{i = 0}^{n} \\hat p_i^2 - 2 \\dfrac{n \\sum\\limits_{i=1}^{n} \\hat p_i^2 - 1}{(n-1)h} = \\dfrac{2}{(n-1)h} - \\dfrac{n+1}{(n-1)h} \\sum\\limits_{i=1}^{n} \\hat p_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot CV estimates $\\hatJ(h)$ that you obtained during selection. Mark optimal bandwidth $h_{cv}$ and report $h_{cv}$ and $\\hatJ(h_{cv})$ (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(h_list, cv_list, color = 'b')\n",
    "plt.vlines(h_opt, -0.2, 1, colors='r', linestyles = 'dashed', label='h_opt')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('J(h)')\n",
    "plt.grid(True)\n",
    "plt.title('Estimates J(h)')\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal bandwidth 0.84583 with risk -0.16162 and optimal bins count = 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal bandwidth \" + str(round(h_opt, 5)) + \" with risk \" + \n",
    "      str(round(J_est_min, 5)) + \" and optimal bins count = \" + str(bins_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Recall the approximation to $MISE$ from Lecture 7, slides 11-13. Assume an Oracle calculated the integral of the squared derivative of the true density for you and the value is $0.0804924$. What will be the approximate optimal bandwidth $h^*$? Find CV estimate of $J$ for this value of bandwidth and compare it to the one found in part 1 (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal MISE in h = 0.9067 with risk estimate -0.15077 and bins num 11\n"
     ]
    }
   ],
   "source": [
    "p_sq_int = 0.0804924\n",
    "h_star = ((6 / p_sq_int) ** (1/3)) / (n_samples ** (1/3))\n",
    "J_star = J_estimate(h_star, samples)\n",
    "bins_star = bins_count(samples, h_star)\n",
    "print(\"Minimal MISE in h = \" + str(round(h_star, 5)) + \" with risk estimate \" + \n",
    "      str(round(J_star, 5)) + \" and bins num \" + str(bins_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This value is a bit larger (and with the larger estimate $J(h)$) but quite close to the one we found in the part 1. Bins number remains the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `np.histogram` has some built-in methods of selecting bandwidth. Compare some of them with your result and theoretical approximation (again, using CV estimate of $J$) (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For method auto h = 9, bandwidth: 1.03139 and risk estimate -0.15094\n",
      "For method fd h = 6, bandwidth: 1.54708 and risk estimate -0.14745\n",
      "For method scott h = 7, bandwidth: 1.32607 and risk estimate -0.15741\n",
      "For method rice h = 11, bandwidth: 0.84386 and risk estimate -0.162\n",
      "For method sturges h = 9, bandwidth: 1.03139 and risk estimate -0.15094\n",
      "For method sqrt h = 11, bandwidth: 0.84386 and risk estimate -0.162\n"
     ]
    }
   ],
   "source": [
    "method = ['auto', 'fd', 'scott', 'rice', 'sturges', 'sqrt']\n",
    "\n",
    "for i in range(6):\n",
    "    hist, bin_edges = np.histogram(samples, bins = method[i], range = (np.min(samples), np.max(samples)), density = False)\n",
    "    bw_np = get_bandwidth(samples, len(bin_edges))\n",
    "    J_np = J_estimate(bw_np, samples)\n",
    "    print('For method ' + str(method[i]) + ' h = ' + str(len(bin_edges)) + \n",
    "          ', bandwidth: ' + str(round(bw_np, 5)) + ' and risk estimate ' + str(round(J_np, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can observe that methods 'rice' and 'sqrt' give us the lowest J estimate.\n",
    "##### If we just use $ np.histogram\\_bin\\_edges$ without any method of bins counting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bins num: 11, bandwidth for it 0.84386 and J estimate -0.162\n"
     ]
    }
   ],
   "source": [
    "p = np.histogram_bin_edges(samples)\n",
    "bins_np = p.shape[0]\n",
    "bw_np = get_bandwidth(samples, bins_opt)\n",
    "J_np = J_estimate(bw_np, samples)\n",
    "print('Bins num: ' + str(bins_np) + ', bandwidth for it ' + str(round(bw_np, 5)) +\n",
    "                                                            ' and J estimate ' + str(round(J_np, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will use the same sample, but employ the kernel density estimation method. We can use the same proxy expression to optimise the risk:\n",
    "\\begin{gather*}\n",
    "J(h) = \\int \\hatp^2 \\lp x;\\xs{n} \\rp dx - 2 \\int \\hatp \\lp x;\\xs{n} \\rp p(x) dx\n",
    "\\end{gather*}\n",
    "\n",
    "In out case of kernel estimator, we can obtain:\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{1}{hn^2}\\Sum_{i = 1}^n\\Sum_{j = 1}^n K^{(2)}\\lp\\frac{x_i - x_j}{h}\\rp + \\frac{2 K(0)}{nh}, \\label{eq3}\\tag{3}\n",
    "\\end{gather*}\n",
    "where\n",
    "\\begin{gather*}\n",
    "K^{(2)}(x) = K^*(x) - 2K(x), \\quad K^*(x) = \\int K(x - y) K(y) dy.\n",
    "\\end{gather*}\n",
    "\n",
    "You can use [kernel density estimation from sklearn](http://scikit-learn.org/stable/modules/density.html). Your task is:\n",
    "1. Build a kernel density estimate given the sample from before, tune bandwidth using leave-one-out CV. Try two different kernels of your choice. Use formula \\ref{eq3} for your estimates (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_min, sample_max = np.min(samples), np.max(samples)\n",
    "delta = sample_max - sample_min\n",
    "grid = np.linspace(sample_min, sample_max, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For risk estimate I took three kernels: gaussian, rectangular and Epanechnikov. I computed their convolutions using Wolfram Alpha (functions kernel_i_conv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel = lambda x, y, h: rbf_kernel(x, y, 0.5 / h**2) / (2 * np.pi)**(x.shape[1] / 2)\n",
    "kernel_1 = lambda x: np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n",
    "kernel_2 = lambda x: 1/2 * (np.abs(x) < 1)\n",
    "kernel_3 = lambda x: 3/4 * (1 - x**2) * (np.abs(x) < 1)\n",
    "\n",
    "def kernel_1_conv(x):\n",
    "    return np.exp(-x**2 / 4) / (2 * np.sqrt(np.pi)) - 2 * kernel_1(x)\n",
    "\n",
    "def kernel_3_conv(x):\n",
    "    res = 9 / (30 * 16)\n",
    "    if (x > 0 and x < 2):\n",
    "        res *=  -(x - 2)**3 * (x**2 + 6*x + 4)\n",
    "        return res - 2 * kernel_3(x)\n",
    "    elif (x > -2 and x <= 0):\n",
    "        res *= (x + 2)**3 * (x**2 - 6*x + 4)\n",
    "        return res - 2 * kernel_3(x)\n",
    "    else:\n",
    "        return (- 2) * kernel_3(x)\n",
    "    \n",
    "def kernel_2_conv(x):\n",
    "    if (x > 0 and x < 2):\n",
    "        return 1/4 * (2 - x) - 2 * kernel_2(x)\n",
    "    elif (x > -2 and x <= 0.):\n",
    "        return 1/4 * (x + 2) - 2 * kernel_2(x)\n",
    "    else:\n",
    "        return (-2) * kernel_2(x)\n",
    "    \n",
    "def J_kernel_estimate(kernel, kernel_conv, X, h):\n",
    "    n = len(X)\n",
    "    sample_min, sample_max = np.min(X), np.max(X)\n",
    "    bins = bins_count(X, h)\n",
    "    mask = np.ones(n, dtype=np.bool_)\n",
    "    X = X[:, None]\n",
    "    res = 2 * kernel(0) / (n * h)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            res += kernel_conv((X[i] - X[j]) / h) / (h * n**2)\n",
    "    return res\n",
    "\n",
    "    #for i in range(n):\n",
    "    #    mask[i] = False\n",
    "    #    p = KernelDensity(bandwidth=h).fit(sample[mask])\n",
    "    #    mask[i] = True\n",
    "    #    summ += np.exp(p.score_samples(sample[i].reshape(1, 1)))\n",
    "    #summ = 2 * summ / n * h\n",
    "    #p = KernelDensity(bandwidth=h).fit(sample[mask]).score_samples(np.linspace(sample_min, sample_max, 101)[:, None])\n",
    "    #p /= p.sum()\n",
    "    #summ += np.sum(p**2) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid search with leave-one-out cross-validations with two kernels: gaussian and Epanechnikov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gaussian kernel:\n",
      "Bandwidth = 0.49370478528390027\n",
      "J estimate for this bandwidth: -0.1607864944650761\n",
      "---------------------------------------------\n",
      "For Epanechnikov kernel:\n",
      "Bandwidth = 1.135733358343105\n",
      "J estimate for this bandwidth: -0.1618706846455322\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "params = {'bandwidth': np.logspace(-1, 2, 200)}\n",
    "kernels = ['gaussian', 'epanechnikov']\n",
    "kde = []\n",
    "for kernel in kernels:\n",
    "    grid_search = GridSearchCV(KernelDensity(kernel = kernel), params, cv=LeaveOneOut())\n",
    "    grid_search.fit(samples.reshape(-1, 1))\n",
    "    kde.append(grid_search.best_estimator_)\n",
    "    #p1 = np.exp(kde.score_samples(samples.reshape(-1, 1)))\n",
    "    if (kernel =='gaussian'):\n",
    "        print('For gaussian kernel:')\n",
    "    else:\n",
    "        print('For Epanechnikov kernel:')\n",
    "    print('Bandwidth =', kde[-1].bandwidth)\n",
    "    if (kernel == 'gaussian'):\n",
    "        print('J estimate for this bandwidth:', J_kernel_estimate(kernel_1, kernel_1_conv, samples, kde[-1].bandwidth)[0])\n",
    "    else:\n",
    "        print('J estimate for this bandwidth:', J_kernel_estimate(kernel_3, kernel_3_conv, samples, kde[-1].bandwidth)[0])\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot CV estimates $\\hatJ(h)$ that you obtained during selection. Mark optimal bandwidth $h_{cv}$ and report $h_{cv}$ and $\\hatJ(h_{cv})$. What kernel worked better in terms of estimated risk? (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = np.logspace(-3, 1, 100)\n",
    "cv_list = [J_kernel_estimate(kernel_1, kernel_1_conv, samples, h_i) for h_i in h_list]\n",
    "j_min = np.argmin(cv_list)\n",
    "h_opt = h_list[j_min]\n",
    "J_est_min = cv_list[j_min][0]\n",
    "\n",
    "print('For gaussian:')\n",
    "print('h = ' + str(round(h_opt, 5)) + ', risk estimate ' + \n",
    "                   str(round(J_est_min, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list_2 = np.logspace(-3, 1.5, 150)\n",
    "cv_list_2 = [J_kernel_estimate(kernel_2, kernel_2_conv, samples, h_i) for h_i in h_list_2]\n",
    "j_min_2 = np.argmin(cv_list_2)\n",
    "h_opt_2 = h_list_2[j_min_2]\n",
    "J_est_min_2 = cv_list_2[j_min_2][0]\n",
    "\n",
    "print('For rectangular:')\n",
    "print('h = ' + str(round(h_opt_2, 5)) + ', risk estimate ' + \n",
    "                   str(round(J_est_min_2, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list_3 = np.logspace(-3, 1.5, 150)\n",
    "cv_list_3 = [J_kernel_estimate(kernel_3, kernel_3_conv, samples, h_i) for h_i in h_list_3]\n",
    "j_min_3 = np.argmin(cv_list_3)\n",
    "h_opt_3 = h_list_3[j_min_3]\n",
    "J_est_min_3 = cv_list_3[j_min_3][0]\n",
    "\n",
    "print('For Epanechnikov:')\n",
    "print('h = ' + str(round(h_opt_3, 5)) + ', risk estimate ' + \n",
    "                   str(round(J_est_min_3, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(h_list, cv_list, color = 'b')\n",
    "plt.vlines(h_opt, -0.2, 2.5, colors='r', linestyles = 'dashed', label='h_opt')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('J(h)')\n",
    "plt.grid(True)\n",
    "plt.title('Estimates J(h) with gaussian kernel')\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(h_list_2, cv_list_2, color = 'b')\n",
    "plt.vlines(h_opt_2, -0.2, 5, colors='r', linestyles = 'dashed', label='h_opt')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('J(h)')\n",
    "plt.grid(True)\n",
    "plt.title('Estimates J(h) with rectangular kernel')\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(h_list_3, cv_list_3, color = 'b')\n",
    "plt.vlines(h_opt_3, -0.2, 6, colors='r', linestyles = 'dashed', label='h_opt')\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('J(h)')\n",
    "plt.grid(True)\n",
    "plt.title('Estimates J(h) with Epanechnikov kernel')\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In terms of risk approximated rectangular kernel worked best, which is surprising because it has the simpliest form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_ar = np.logspace(-1, 1.3, 100)\n",
    "h_ar = np.linspace(-2.5, 7.5, 500)\n",
    "h_ar = np.array(h_ar)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(samples, bins=11, density=True);\n",
    "for kernel, kde_ in zip(kernels, kde):\n",
    "    density = KernelDensity(kernel=kernel, bandwidth=kde_.bandwidth).fit(samples.reshape(-1, 1))\n",
    "    plt.plot(h_ar, np.exp(density.score_samples(h_ar[:, None])), label=kernel, linewidth=4)\n",
    "plt.legend()\n",
    "plt.title('Kernel density estimate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Recall the approximation to $MISE$ from Lecture 7, slide 20. Assume an Oracle calculated the integral of the squared second derivative of the true density for you and the value is $0.127529$. What will be the approximate optimal bandwidth $h^*$? Find CV estimate of $J$ for this value of bandwidth and compare it to the one found in part 1 (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaussian kernel: \n",
    "\n",
    "$$\n",
    "K(x) = \\dfrac{\\exp^{-x^2}}{\\sqrt{2\\pi}} \n",
    "$$\n",
    "Find an integral from $-\\infty$ to $\\infty$ of its square:\n",
    "$$\n",
    "\\int K^2(x) dx = \\dfrac{1}{2 \\sqrt{\\pi}} = 0.282095\n",
    "$$\n",
    "And an integral $\\int x^2 K(x)dx$:\n",
    "$$\n",
    "\\int x^2 K(x) dx = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2_int = 0.127529\n",
    "k_2_int = 0.282095\n",
    "h_star = (1 / n_samples * k_2_int / p_2_int) ** (1/5)\n",
    "print('h*:', h_star)\n",
    "print('Risk estimated:', J_kernel_estimate(kernel_1, kernel_1_conv, samples, h_star)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Same for Epanechnikov kernel (integrals are counted through Wolfram Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_star_epn = (1 / ((1/5)**2 * n_samples) * 3/5 / p_2_int) ** (1/5)\n",
    "print('h*:', h_star_epn)\n",
    "print('Risk estimated:', J_kernel_estimate(kernel_3, kernel_3_conv, samples, h_star_epn)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Non-parametric Regression (5.5 pt)\n",
    "\n",
    "In this task you will apply non-parametric regression to airport statistics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "Load dataset from `airport_operations_report.csv`. Use `General Aviation: Total Operations` as target(dependent) variable and `[Air Carrier Operations, General Aviation: Local Operations]` as covariates(independent variables). Divide $1^{st}$ covariate by `1000`, $2^{nd}$ by `100` and target by `1000`. Make 2D grid for covariates from minimum to maximum values with `100` steps along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "df = pd.read_csv('airport_operations_report.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "X = np.array([df['Air Carrier Operations'].values, df['General Aviation: Local Operations'].values]).T\n",
    "print(X.shape)\n",
    "X[:, 0] /= 1000\n",
    "X[:, 1] /= 100\n",
    "#X['Air Carrier Operations'][:] /= 1000\n",
    "#X['General Aviation: Local Operations'][:] /= 100\n",
    "y = np.array(df['General Aviation: Total Operations'].values)\n",
    "print(y.shape)\n",
    "y[:] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "cov_2 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "xx, yy = np.meshgrid(cov_1, cov_2)\n",
    "\n",
    "grid = np.stack([xx, yy])\n",
    "grid = np.transpose(grid, (1, 2, 0))\n",
    "grid = grid.reshape(-1, 2)\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model (1 pt)\n",
    "\n",
    "Perform a nonparametric regression to fit the model $Y = f(x)+\\varepsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelReg(endog = y, exog = X, var_type='cc', reg_type='lc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict values for created grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_numbers - number of steps in grid: (100, 100)\n",
    "step_numbers = (100, 100)\n",
    "\n",
    "target_pred, margins = model.fit(grid)\n",
    "target_pred = target_pred.reshape(*step_numbers)\n",
    "grid = grid.reshape(*step_numbers, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is function to visualize 3D surfaces. You can modify it if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface_function(x1, x2, y, minmax_values=None, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize = (8, 8))\n",
    "        ax = fig.gca(projection='3d')\n",
    "\n",
    "        surf = ax.plot_surface(x1, x2, y, cmap=cm.rainbow, linewidth=0.5, alpha=0.5, edgecolor=\"k\")\n",
    "        if minmax_values is not None:\n",
    "            ax.set_xlim3d(*minmax_values[0])\n",
    "            ax.set_ylim3d(*minmax_values[1])\n",
    "    else:\n",
    "        surf = ax.plot_surface(x1, x2, y, cmap=cm.bwr, linewidth=0.5, alpha=0.3, edgecolor=\"k\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot estimated surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target - dependent variable\n",
    "# grid - grid for covariates\n",
    "# sample - covariates\n",
    "\n",
    "sample_min = [np.min(grid[:, :, 0]), np.min(grid[:, :, 1])]\n",
    "sample_max = [np.max(grid[:, :, 0]), np.max(grid[:, :, 1])]\n",
    "\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred, minmax_values=np.vstack((sample_min, sample_max)).T)\n",
    "ax.scatter(X[:, 0], X[:, 1], y, marker=\"o\", s=25, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal bandwidth by KernelReg:', model.bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate confidence intervals (2 pt)\n",
    "\n",
    "Construct 95\\% confidence bands for your estimate. To estimate error variance for 1D covariate, you sort your sample and subtract target values in the nearest points. For 2D covariate, you should, for each point, calculate the difference in target values between a point and its nearest neighbor(in euclidian metric). To get power for quantile of standard normal distribution, which is $\\frac{w}{b-a}$ for 1D, estimate it per coordinate and multiply: $\\prod_{i=1}^2 \\frac{w_i}{b_i-a_i}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_2d(x, y, h):\n",
    "    return rbf_kernel(x / h, y / h, 0.5) / (2 * np.pi)**(x.shape[1] / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_se(grid, sample, target, h, alpha=0.05):\n",
    "    n = target.shape[0]\n",
    "    sigma_hat2 = 0\n",
    "    mask = np.ones(n, dtype=np.bool_)\n",
    "    for i in range(n):\n",
    "        mask[i] = False\n",
    "        neigh = NearestNeighbors(n_neighbors = 1)\n",
    "        neigh.fit(sample[mask])\n",
    "        ind = neigh.kneighbors([sample[i]])[1][0][0]\n",
    "        if (ind >= i):\n",
    "            ind += 1\n",
    "        mask[i] = True\n",
    "        sigma_hat2 += (target[i] - target[ind])**2\n",
    "    sigma_hat2 =  sigma_hat2 * (1 / (2 * (n-1)))\n",
    "    \n",
    "    delta_1 = np.max(sample[:, 0]) - np.min(sample[:, 0])\n",
    "    delta_2 = np.max(sample[:, 1]) - np.min(sample[:, 1])\n",
    "\n",
    "    kernel_values = kernel_2d(grid, sample, h)\n",
    "    kernel_sum = np.sum(kernel_values, axis=1, keepdims=True)\n",
    "    kernel_weights = kernel_values / kernel_sum\n",
    "    \n",
    "    se = np.sqrt(np.sum(kernel_weights**2, axis=1) * sigma_hat2)\n",
    "    m = (9 * h[0] * h[1]) / (delta_1 * delta_2)\n",
    "    q = sps.norm.ppf((1 + (1 - alpha) ** m) / 2)\n",
    "    return q * se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.reshape(100, 100, 2)\n",
    "grid_1 = grid.reshape(-1, 2)\n",
    "qse = prediction_se(grid_1, X, y, model.bw)\n",
    "qse = qse.reshape(*step_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_min = [np.min(grid[:, :, 0]), np.min(grid[:, :, 1])]\n",
    "sample_max = [np.max(grid[:, :, 0]), np.max(grid[:, :, 1])]\n",
    "\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred, minmax_values=np.vstack((sample_min, sample_max)).T)\n",
    "ax.scatter(X[:, 0], X[:, 1], y, marker=\"o\", s=25, c=\"r\")\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred + qse, ax=ax)\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred - qse, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal bandwidth (2 pt)\n",
    "\n",
    "Use cross-validation to estimate the bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_regression(sample, target, h):\n",
    "    n = sample.shape[0]\n",
    "    mask = np.ones(n, dtype=np.bool_)\n",
    "    bw = h\n",
    "    summ = 0\n",
    "    for i in range(n):\n",
    "        mask[i] = False\n",
    "        nw = KernelReg(target[mask], sample[mask], var_type=\"cc\", reg_type=\"lc\", bw=bw)\n",
    "        mask[i] = True\n",
    "        summ += (target[i] - nw.fit(sample[i].reshape(2, 1))[0])**2\n",
    "    return summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make grid with logarithm step to find best bandwidth, from `0.1` to `10` with `21` steps along each dimension. Visualize obrained results with `plot_surface_function`. Print values for optimal bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(y)\n",
    "X_copy = np.vstack((X[order, 0], X[order, 1]))\n",
    "#np.concatenate((X[order, 0], X[order, 1]), axis=1)\n",
    "X_copy = np.transpose(X_copy)\n",
    "print(X_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = np.logspace(-1, 1, 21)\n",
    "hxx, hyy = np.meshgrid(h_list, h_list)\n",
    "\n",
    "h_grid = np.stack([hxx, hyy])\n",
    "h_grid = np.transpose(h_grid, (1, 2, 0))\n",
    "h_grid = h_grid.reshape(-1, 2)\n",
    "\n",
    "cv_hist = np.array([cross_validation_regression(X_copy, y[order], h_) for h_ in (h_grid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_min = [np.min(h_2d[:, 0]), np.min(h_2d[:, 1])]\n",
    "#h_max = [np.max(h_2d[:, 0]), np.max(h_2d[:, 1])]\n",
    "\n",
    "fig1 = plt.figure(figsize=(8, 8))\n",
    "ax = fig1.gca(projection='3d')\n",
    "ax.plot_trisurf(h_grid[:len(cv_hist), 0], h_grid[:len(cv_hist), 1], cv_hist.reshape(-1), cmap=plt.cm.Spectral)\n",
    "fig1.show()\n",
    "#ax = plot_surface_function(h_2d[:, 0], h_2d[:, 1], cv_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot surface with estimated bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_opt = h_grid[cv_hist.argmin()]\n",
    "print('Optimal bandwidth for 21 points per dimension:', h_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelReg(endog = y, exog = X, var_type='cc', reg_type='lc', bw=h_opt)\n",
    "step_numbers = (100, 100)\n",
    "\n",
    "target_pred, margins = model.fit(grid.reshape(-1, 2))\n",
    "target_pred = target_pred.reshape(*step_numbers)\n",
    "grid = grid.reshape(*step_numbers, 2)\n",
    "\n",
    "sample_min = [np.min(grid[:, :, 0]), np.min(grid[:, :, 1])]\n",
    "sample_max = [np.max(grid[:, :, 0]), np.max(grid[:, :, 1])]\n",
    "\n",
    "ax = plot_surface_function(grid[:, :, 0], grid[:, :, 1], target_pred, minmax_values=np.vstack((sample_min, sample_max)).T)\n",
    "ax.scatter(X[:, 0], X[:, 1], y, marker=\"o\", s=25, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = np.logspace(-1, 1, 51)\n",
    "hxx, hyy = np.meshgrid(h_list, h_list)\n",
    "\n",
    "h_grid = np.stack([hxx, hyy])\n",
    "h_grid = np.transpose(h_grid, (1, 2, 0))\n",
    "h_grid = h_grid.reshape(-1, 2)\n",
    "\n",
    "cv_hist = np.array([cross_validation_regression(X_copy, y[order], h_) for h_ in (h_grid)])\n",
    "\n",
    "print('Optimal bandwidth for 51 points per dimension:', h_grid[cv_hist.argmin()])\n",
    "print('Risk for optimal value:', cv_hist.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (0.5 pt)\n",
    "\n",
    "Write your conclusions about conducted experiments.\n",
    "\n",
    "##### We tried two approaches for this regression problem: KernelReg and custom cross-validation.\n",
    "##### From the 3d plot it seems that most of the points are located inside the confidence bands (if some of the point are outside it should be fine because it's just 95% confidence interval).\n",
    "##### The optimal bandwidth found with usage of KernelReg ($[0.5371977, 0.55854071]$) is quite close to the values found by cross-validation method ($[0.50118723, 0.50118723]$) and they become even closer if we take more frequent grid for cross-validation (not 21 points per dimension as required, but 51 points, for example, and get $[0.52480746, 0.57543994]$). \n",
    "##### So we can conclude that these methods are close in their result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Model Selection (5.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will select model(choose set of covariates) using AIC criteria and forward/backward stepwise regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "Read dataset `pacn_wrcc.csv`. The target variable is `ly Solar Rad.`, others are covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pacn_wrcc.csv\")\n",
    "X_aic = np.array(df.drop(columns=['ly Solar Rad.']).values)\n",
    "y_aic = np.array(df['ly Solar Rad.'].values)\n",
    "print(X_aic.shape)\n",
    "print(y_aic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale covariates to range `[0, 1]` and add bias column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler as scaler\n",
    "\n",
    "sc = scaler()\n",
    "X_aic = sc.fit_transform(X_aic)\n",
    "#X_aic = np.interp(X_aic, (np.min(X_aic), np.max(X_aic)), (0, 1))\n",
    "X_aic = np.transpose(np.vstack([np.transpose(X_aic), np.ones(y_aic.shape[0])]))\n",
    "print(X_aic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIC and sample variance (2 pt) \n",
    "\n",
    "Estimate sample variance of error with full set of covariates. Take into account number of dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat\\sigma^2 = \\dfrac{1}{n-k}\\sum\\limits_{i=1}^{n} \\hat\\varepsilon_i^2\n",
    "$$\n",
    "$$\n",
    "\\hat\\varepsilon = \\hat\\beta X - Y\n",
    "$$\n",
    "$$\n",
    "k = shape(X^T X)[0]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_2 = 0\n",
    "nn = (X_aic.T @ X_aic).shape[0]\n",
    "beta_hat = np.linalg.inv(X_aic.T @ X_aic) @ X_aic.T @ y_aic #0.0001 * np.ones((nn, nn))\n",
    "eps = X_aic @ beta_hat - y_aic\n",
    "k = (X_aic.T @ X_aic).shape[0]\n",
    "\n",
    "for i in range(eps.shape[0]):\n",
    "    sigma_2 += eps[i]**2\n",
    "sigma_2 = sigma_2 / (eps.shape[0] - k)\n",
    "print(sigma_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement estimate of AIC for given covariates, target and error variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic(X, y, sigma2):\n",
    "    n = X.shape[0]\n",
    "    #nn = (X.T @ X).shape[0]\n",
    "    beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y #0.0001 * np.ones((nn, nn))\n",
    "    l_s = - n * np.log(np.sqrt(sigma2 * 2 * np.pi))\n",
    "    l_s = l_s - 1 / (2 * sigma2) * (np.linalg.norm(y - X @ beta_hat) ** 2)\n",
    "    return l_s - X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward stepwise regression (1 pt)\n",
    "\n",
    "Implement forward stepwise regression. Save the order in which covariates are added to feature set and AIC values for those feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def forward_stepwise_regression(X, y, sigma2, alpha=0.1):\n",
    "#    features = []\n",
    "#    aic_list = []\n",
    "#    is_improvement = True\n",
    "#    while (is_improvement):\n",
    "#        size = len(features)\n",
    "#        for i in range(X.shape[1]):\n",
    "#            diff = []\n",
    "#            if (i not in features):\n",
    "#                diff.append(aic(X[:, features + [i]], y, sigma2) - aic(X[:, features], y, sigma2))\n",
    "#                if (np.max(diff) > 0):\n",
    "#                    features.append(np.argmax(diff))\n",
    "#                    aic_list.append(aic(X[:, features], y, sigma2))\n",
    "#        if (len(features) == size):\n",
    "##            is_improvement = False\n",
    "#    return features, aic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_stepwise_regression(X, y, sigma2):\n",
    "    features = []\n",
    "    aic_list = []\n",
    "    idxes = [i for i in range(X.shape[1])]\n",
    "    eps = -1e10\n",
    "    \n",
    "    while(idxes):\n",
    "        size = len(idxes)\n",
    "        aic_cur = []\n",
    "        for i in idxes:\n",
    "            features_1 = features + [i]\n",
    "            aic_cur.append(aic(X[:, features_1], y, sigma2))\n",
    "        value = np.max(aic_cur)\n",
    "        if (value > eps):\n",
    "            eps = value\n",
    "            aic_list.append(value)\n",
    "            features = features + [idxes[np.argmax(aic_cur)]]\n",
    "            idxes.remove(idxes[np.argmax(aic_cur)])\n",
    "            #print(idxes)\n",
    "        if (len(idxes) == size):\n",
    "            break\n",
    "    return features, aic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, aic_list = forward_stepwise_regression(X_aic, y_aic, sigma_2)\n",
    "for a, aic_value in enumerate(aic_list, 1):\n",
    "    print(f\"AIC {aic_value:.3f} + {features[:a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward stepwise regression (1 pt)\n",
    "\n",
    "Implement backward stepwise regression. Save the order in which covariates are removed from feature set and AIC values for those feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_stepwise_regression(X, y, sigma2, alpha=0.1):\n",
    "    features = []\n",
    "    aic_list = []\n",
    "    \n",
    "    idxes = [i for i in range(X.shape[1])]\n",
    "    eps = -1e10\n",
    "    \n",
    "    while(idxes):\n",
    "        size = len(idxes)\n",
    "        aic_cur = []\n",
    "        for i in idxes:\n",
    "            features_1 = idxes.copy()\n",
    "            features_1.remove(i)\n",
    "            aic_cur.append(aic(X[:, features_1], y, sigma2))\n",
    "        value = np.max(aic_cur)\n",
    "        if (value > eps):\n",
    "            eps = value\n",
    "            aic_list.append(value)\n",
    "            features = features + [idxes[np.argmax(aic_cur)]]\n",
    "            idxes.remove(idxes[np.argmax(aic_cur)])\n",
    "        if (len(idxes) == size):\n",
    "            break\n",
    "    return features, aic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, aic_list = backward_stepwise_regression(X_aic, y_aic, sigma_2)\n",
    "for a, aic_value in enumerate(aic_list, 1):\n",
    "    print(f\"AIC {aic_value:.3f} -{features[:a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruteforce (1 pt)\n",
    "\n",
    "Find best sets of covariates and their AIC for all sizes of feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "#for i in combinations(range(X_aic.shape[1]), 13):\n",
    "#    print(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteforce(X, y, sigma2, alpha=0.):\n",
    "    features = []\n",
    "    aic_list = []\n",
    "    all_sets = []\n",
    "    for k in range(1, X.shape[1]):\n",
    "        for subset in combinations(range(X.shape[1]), k):\n",
    "            all_sets.append(list(subset))\n",
    "    eps = -1e10\n",
    "\n",
    "    for subset in all_sets:\n",
    "        value = aic(X[:, subset], y, sigma2)\n",
    "        if (value > eps):\n",
    "            eps = value\n",
    "            aic_list.append(value)\n",
    "            features.append(subset)\n",
    "    return features, aic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, aic_list = bruteforce(X_aic, y_aic, sigma_2)\n",
    "for a, aic_value in enumerate(aic_list):\n",
    "    print(f\"AIC {aic_value:.3f} -{features[a]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (0.5 pt)\n",
    "\n",
    "Write your conclusions about conducted experiments.\n",
    "\n",
    "##### In my case, the results for all of these 3 methods coincide which is a good sign. I can't say for sure whether they always do because as far as I understand, ideally, for more efficient model selection one should use forward/backward methods together: first, select features with forward method and then delete some of them with backward method. Still, in our case the backward method rejected exactly the same features that weren't chosen in forward, so they are coordinated. The AIC value has improved in almost 3 times.\n",
    "##### The last thing that should be noticed that the bias column has been thrown out of the model. In our problem it is not necessary for the fitting, although sometimes it is, for example, for fearure vector $x = [1, 2, 3, 4]$ and $y = [3, 5, 7, 9]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Local regression (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-3.        , -2.93939394, -2.87878788, -2.81818182, -2.75757576,\n",
    "       -2.6969697 , -2.63636364, -2.57575758, -2.51515152, -2.45454545,\n",
    "       -2.39393939, -2.33333333, -2.27272727, -2.21212121, -2.15151515,\n",
    "       -2.09090909, -2.03030303, -1.96969697, -1.90909091, -1.84848485,\n",
    "       -1.78787879, -1.72727273, -1.66666667, -1.60606061, -1.54545455,\n",
    "       -1.48484848, -1.42424242, -1.36363636, -1.3030303 , -1.24242424,\n",
    "       -1.18181818, -1.12121212, -1.06060606, -1.        , -0.93939394,\n",
    "       -0.87878788, -0.81818182, -0.75757576, -0.6969697 , -0.63636364,\n",
    "       -0.57575758, -0.51515152, -0.45454545, -0.39393939, -0.33333333,\n",
    "       -0.27272727, -0.21212121, -0.15151515, -0.09090909, -0.03030303,\n",
    "        0.03030303,  0.09090909,  0.15151515,  0.21212121,  0.27272727,\n",
    "        0.33333333,  0.39393939,  0.45454545,  0.51515152,  0.57575758,\n",
    "        0.63636364,  0.6969697 ,  0.75757576,  0.81818182,  0.87878788,\n",
    "        0.93939394,  1.        ,  1.06060606,  1.12121212,  1.18181818,\n",
    "        1.24242424,  1.3030303 ,  1.36363636,  1.42424242,  1.48484848,\n",
    "        1.54545455,  1.60606061,  1.66666667,  1.72727273,  1.78787879,\n",
    "        1.84848485,  1.90909091,  1.96969697,  2.03030303,  2.09090909,\n",
    "        2.15151515,  2.21212121,  2.27272727,  2.33333333,  2.39393939,\n",
    "        2.45454545,  2.51515152,  2.57575758,  2.63636364,  2.6969697 ,\n",
    "        2.75757576,  2.81818182,  2.87878788,  2.93939394,  3.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
    "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a sample of pairs $(X_i, Y_i)$. The output variable appers to be binary. We know a model for this type of data: logistic regression. In this model we assume the log-odds of the Bernoulli output variable to be a linear function:\n",
    "$$\\mathbb{P}(Y_i = 1 | X_i = x_i) = p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}$$\n",
    "Lets fit a standard logistic regression to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty='none')\n",
    "logreg.fit(X.reshape(-1, 1), Y)\n",
    "p_pred_global = logreg.predict_proba(X.reshape(-1, 1))[:, 1]\n",
    "print(f\"Coefficients of the fitted logistic regression model: b0={logreg.intercept_[0]}, b1={logreg.coef_[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, '*', label='Observations')\n",
    "plt.plot(X, p_pred_global, label='Logictic regression')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dependence of the outcome on $x$ looks more complex than estimated using logistic regression. We will try to improve it with local regression methods instead.\n",
    "Instead of a global model, we will approximate conditinal probability of the positive class in a neighbourhood of $x$ with:\n",
    "$$p(u) \\approx \\frac{e^{\\beta_0 + \\beta_1 (u-x) }}{1 + e^{\\beta_0 + \\beta_1 (u-x)}}, $$\n",
    "for $u$ close to $x$. The coefficients in this case will depend on $x$. To find them, we will have to fit a (slightly different) model at each new query point $x_{new}$ (point where we want to predict $Y$). We will also need to introduce a notion of 'closenes' of points across $x$ - for this we can use our familiar *kernel functions*. Bringing everything together, we introduce the following local loglikelihood for our proposed model:\n",
    "$$\\ell_x(\\beta) = \\sum\\limits_{i=1}^{n} K\\left(\\frac{x-X_i}{h}\\right)\\ell\\left(Y_i, \\beta_0 + \\beta_1 \\left(X_i - x\\right)\\right) = \\sum\\limits_{i=1}^{n} K\\left(\\frac{x-X_i}{h}\\right) \\left(Y_i \\left(\\beta_0 + \\beta_1 \\left(X_i - x\\right)\\right) - \\log \\left( 1 + e^{\\beta_0 + \\beta_1 \\left(X_i - x\\right)} \\right) \\right),$$\n",
    "$$\\ell(y, z) = yz - \\log \\left(1 + e^z \\right).$$\n",
    "Here $\\ell(y, z)$ is a log-likelohood for a single Bernoulli with log-dds ratio $z = \\frac{p}{1-p}$. The local log-likelihood $\\ell_x(\\beta)$ should be optimized numerically to each new point $x$ to obtain $\\widehat{\\beta}(x)=(\\widehat{\\beta_0}(x), \\widehat{\\beta_1}(x))$. Then, we can predict $Y$ using the logistic model:\n",
    "$$ \\mathbb{P}(Y=1 | X=x) = \\frac{e^{\\beta_0(x)}}{1 + e^{\\beta_0(x)}}.$$\n",
    "Your task is:\n",
    "1. Implement the proposed local logistic regression approach using Gaussian kernel. *Hint*: look at additional parameters of the `fit` method of [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from `sklearn` package (2 points)\n",
    "2. Select optimal bandwidth using leave-one-out log-likelihood cross-validation: $\\ell_{cv} = \\sum_i \\ell(Y_i, \\widehat{z}_{-i}(x_i))$, where $\\widehat{z}_{-i}$ is the estimated log-odds ratio without $i$th sample (3 points)\n",
    "3. Now ignore that the output variable is binary. Compare previous results with local linear kernel regression method using `KernelReg` from `statsmodels`. Use leave-one out squared error cross-validation to select the bandwidth (1 bonus point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = lambda x, y, h: rbf_kernel(x, y, 0.5 / h**2) / (2 * np.pi)**(x.shape[1] / 2)\n",
    "\n",
    "def Local_Logistic_Regression(points, X, Y, h):\n",
    "    X_kernel = kernel(points.reshape(-1, 1), X.reshape(-1, 1), h)\n",
    "    proba = np.zeros(points.shape[0])\n",
    "    for i in range(len(points)):\n",
    "        regression = LogisticRegression(solver='lbfgs') #penalty='none' \n",
    "        regression.fit((X - points[i]).reshape(-1, 1), Y, sample_weight = X_kernel[i])\n",
    "        beta = regression.intercept_[0]\n",
    "        proba[i] = np.exp(beta) / (1 + np.exp(beta))\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1 = 1\n",
    "h_2 = 1/3\n",
    "h_3 = 1/6\n",
    "y_pred_1 = Local_Logistic_Regression(X, X, Y, h_1)\n",
    "y_pred_2 = Local_Logistic_Regression(X, X, Y, h_2)\n",
    "y_pred_3 = Local_Logistic_Regression(X, X, Y, h_3)\n",
    "    \n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.plot(X, Y, '*', label='Observations')\n",
    "plt.plot(X, p_pred_global, label='Logictic regression')\n",
    "plt.plot(X, y_pred_1, label='Kernel regression with h = 1')\n",
    "plt.plot(X, y_pred_2, label='Kernel regression with h = 1/3')\n",
    "plt.plot(X, y_pred_3, label='Kernel regression with h = 1/6')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_local_regression(sample, target, h):\n",
    "    n = sample.shape[0]\n",
    "    mask = np.ones(n, dtype=np.bool_)\n",
    "    summ = 0\n",
    "    for i in range(n):\n",
    "        mask[i] = False\n",
    "        #probs = Local_Logistic_Regression(np.array([sample[i]]), sample[mask], \n",
    "        # target[mask], h)\n",
    "        probs = Local_Logistic_Regression(np.array([sample[i]]), sample[mask], \n",
    "                                          target[mask], h)\n",
    "        mask[i] = True\n",
    "        z_i = np.log(probs / (1 - probs))\n",
    "        summ += (target[i] * z_i - np.log(1 + np.exp(z_i))) \n",
    "    return summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_grid = np.logspace(-2, 1, 100) #endpoint=False)\n",
    "cv_hist = np.array([cross_validation_local_regression(X, Y, h_) for h_ in (h_grid)])\n",
    "\n",
    "plt.plot(h_grid, cv_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_grid = np.logspace(-1, 1, 50) #endpoint=False)\n",
    "cv_hist = np.array([cross_validation_local_regression(X, Y, h_) for h_ in (h_grid)])\n",
    "\n",
    "plt.plot(h_grid, cv_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_opt_loc = h_grid[np.argmax(cv_hist)]\n",
    "print('Optimal bandwidth:', h_opt_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now use KernelReg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_regression(sample, target, h):\n",
    "    n = sample.shape[0]\n",
    "    mask = np.ones(n, dtype=np.bool_)\n",
    "    bw = [h]\n",
    "    summ = 0\n",
    "    for i in range(n):\n",
    "        mask[i] = False\n",
    "        nw = KernelReg(target[mask], sample[mask], var_type=\"o\", reg_type=\"lc\", bw=bw)\n",
    "        mask[i] = True\n",
    "        summ += (target[i] - nw.fit(sample[i].reshape(-1, 1))[0])**2\n",
    "    return summ\n",
    "\n",
    "order = np.argsort(Y)\n",
    "\n",
    "h_grid = np.logspace(-4, 0, 100, endpoint=False)\n",
    "cv_hist = np.array([cross_validation_regression(X[order], Y[order], h_) for h_ in (h_grid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h_grid, cv_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_grid = np.logspace(-2, 1, 100)\n",
    "cv_hist = np.array([cross_validation_regression(X[order], Y[order], h_) for h_ in (h_grid)])\n",
    "\n",
    "plt.plot(h_grid, cv_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal bandwidth:', h_grid[np.argmax(cv_hist)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
